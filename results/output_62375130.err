huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  [torch.tensor(ids) for ids in batch["input_ids"]],
/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  [torch.tensor(ids) for ids in batch["attention_mask"]],
/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  [torch.tensor(ids) for ids in batch["labels"]],
  0%|          | 0/435 [00:00<?, ?it/s]  0%|          | 2/435 [00:00<00:31, 13.71it/s]  1%|          | 4/435 [00:00<00:44,  9.61it/s]  1%|▏         | 6/435 [00:00<00:48,  8.76it/s]  2%|▏         | 7/435 [00:00<00:50,  8.54it/s]  2%|▏         | 8/435 [00:00<00:51,  8.37it/s]  2%|▏         | 9/435 [00:01<00:51,  8.20it/s]  2%|▏         | 10/435 [00:01<00:52,  8.08it/s]  3%|▎         | 11/435 [00:01<00:56,  7.57it/s]  3%|▎         | 12/435 [00:01<00:55,  7.63it/s]  3%|▎         | 13/435 [00:01<00:55,  7.67it/s]  3%|▎         | 14/435 [00:01<00:54,  7.68it/s]  3%|▎         | 15/435 [00:01<00:54,  7.70it/s]  4%|▎         | 16/435 [00:01<00:54,  7.71it/s]  4%|▍         | 17/435 [00:02<00:54,  7.73it/s]  4%|▍         | 18/435 [00:02<00:54,  7.71it/s]  4%|▍         | 19/435 [00:02<00:53,  7.72it/s]  5%|▍         | 20/435 [00:02<00:54,  7.62it/s]  5%|▍         | 21/435 [00:02<00:54,  7.64it/s]  5%|▌         | 22/435 [00:02<00:53,  7.66it/s]  5%|▌         | 23/435 [00:02<00:53,  7.65it/s]  6%|▌         | 24/435 [00:03<00:53,  7.66it/s]  6%|▌         | 25/435 [00:03<00:53,  7.66it/s]  6%|▌         | 26/435 [00:03<00:53,  7.63it/s]  6%|▌         | 27/435 [00:03<00:53,  7.63it/s]  6%|▋         | 28/435 [00:03<00:53,  7.62it/s]  7%|▋         | 29/435 [00:03<00:53,  7.63it/s]  7%|▋         | 30/435 [00:03<00:53,  7.62it/s]  7%|▋         | 31/435 [00:03<00:53,  7.56it/s]  7%|▋         | 32/435 [00:04<00:58,  6.93it/s]  8%|▊         | 33/435 [00:04<00:57,  6.96it/s]  8%|▊         | 34/435 [00:04<00:56,  7.06it/s]  8%|▊         | 35/435 [00:04<00:55,  7.19it/s]  8%|▊         | 36/435 [00:04<00:54,  7.30it/s]  9%|▊         | 37/435 [00:04<00:54,  7.37it/s]  9%|▊         | 38/435 [00:04<00:53,  7.42it/s]  9%|▉         | 39/435 [00:05<00:53,  7.44it/s]  9%|▉         | 40/435 [00:05<00:53,  7.45it/s]  9%|▉         | 41/435 [00:05<00:52,  7.47it/s] 10%|▉         | 42/435 [00:05<00:52,  7.48it/s] 10%|▉         | 43/435 [00:05<00:52,  7.48it/s] 10%|█         | 44/435 [00:05<01:05,  5.97it/s] 10%|█         | 45/435 [00:05<01:01,  6.33it/s] 11%|█         | 46/435 [00:06<00:59,  6.56it/s] 11%|█         | 47/435 [00:06<00:57,  6.70it/s] 11%|█         | 48/435 [00:06<00:57,  6.79it/s] 11%|█▏        | 49/435 [00:06<00:55,  6.97it/s] 11%|█▏        | 50/435 [00:06<00:54,  7.10it/s] 12%|█▏        | 51/435 [00:06<00:53,  7.16it/s] 12%|█▏        | 52/435 [00:07<01:25,  4.47it/s] 12%|█▏        | 53/435 [00:07<01:15,  5.04it/s] 12%|█▏        | 54/435 [00:07<01:08,  5.54it/s] 13%|█▎        | 55/435 [00:07<01:04,  5.94it/s] 13%|█▎        | 56/435 [00:07<01:00,  6.22it/s] 13%|█▎        | 57/435 [00:07<00:58,  6.52it/s] 13%|█▎        | 58/435 [00:08<00:56,  6.71it/s] 14%|█▎        | 59/435 [00:08<01:18,  4.77it/s] 14%|█▍        | 60/435 [00:08<01:11,  5.27it/s] 14%|█▍        | 61/435 [00:08<01:05,  5.68it/s] 14%|█▍        | 62/435 [00:08<01:01,  6.03it/s] 14%|█▍        | 63/435 [00:08<00:58,  6.35it/s] 15%|█▍        | 64/435 [00:09<00:56,  6.54it/s] 15%|█▍        | 65/435 [00:09<01:17,  4.75it/s] 15%|█▌        | 66/435 [00:09<01:10,  5.26it/s] 15%|█▌        | 67/435 [00:09<01:05,  5.65it/s] 16%|█▌        | 68/435 [00:09<01:00,  6.03it/s] 16%|█▌        | 69/435 [00:10<00:58,  6.31it/s] 16%|█▌        | 70/435 [00:10<01:28,  4.12it/s] 16%|█▋        | 71/435 [00:10<01:16,  4.77it/s] 17%|█▋        | 72/435 [00:10<01:08,  5.30it/s] 17%|█▋        | 73/435 [00:10<01:03,  5.70it/s] 17%|█▋        | 74/435 [00:11<00:59,  6.05it/s] 17%|█▋        | 75/435 [00:11<01:28,  4.06it/s] 17%|█▋        | 76/435 [00:11<01:16,  4.66it/s] 18%|█▊        | 77/435 [00:11<01:09,  5.18it/s] 18%|█▊        | 78/435 [00:11<01:03,  5.62it/s] 18%|█▊        | 79/435 [00:12<01:04,  5.51it/s] 18%|█▊        | 80/435 [00:12<01:00,  5.82it/s] 19%|█▊        | 81/435 [00:12<00:58,  6.04it/s] 19%|█▉        | 82/435 [00:12<00:56,  6.28it/s] 19%|█▉        | 83/435 [00:12<01:11,  4.94it/s] 19%|█▉        | 84/435 [00:12<01:05,  5.34it/s] 20%|█▉        | 85/435 [00:13<01:01,  5.73it/s] 20%|█▉        | 86/435 [00:13<00:57,  6.04it/s] 20%|██        | 87/435 [00:13<01:14,  4.70it/s] 20%|██        | 88/435 [00:13<01:07,  5.11it/s] 20%|██        | 89/435 [00:13<01:02,  5.54it/s] 21%|██        | 90/435 [00:14<01:20,  4.29it/s] 21%|██        | 91/435 [00:14<01:10,  4.89it/s] 21%|██        | 92/435 [00:14<01:07,  5.08it/s] 21%|██▏       | 93/435 [00:14<01:25,  4.02it/s] 22%|██▏       | 94/435 [00:15<01:14,  4.57it/s] 22%|██▏       | 95/435 [00:15<01:06,  5.08it/s] 22%|██▏       | 96/435 [00:15<01:15,  4.49it/s] 22%|██▏       | 97/435 [00:15<01:06,  5.10it/s] 23%|██▎       | 98/435 [00:15<01:03,  5.27it/s] 23%|██▎       | 99/435 [00:16<01:13,  4.59it/s] 23%|██▎       | 100/435 [00:16<01:05,  5.08it/s] 23%|██▎       | 101/435 [00:16<01:00,  5.48it/s] 23%|██▎       | 102/435 [00:16<01:10,  4.71it/s] 24%|██▎       | 103/435 [00:16<01:04,  5.14it/s] 24%|██▍       | 104/435 [00:17<01:12,  4.55it/s] 24%|██▍       | 105/435 [00:17<01:04,  5.14it/s] 24%|██▍       | 106/435 [00:17<00:59,  5.53it/s] 25%|██▍       | 107/435 [00:17<01:16,  4.28it/s] 25%|██▍       | 108/435 [00:17<01:08,  4.78it/s] 25%|██▌       | 109/435 [00:18<01:23,  3.88it/s] 25%|██▌       | 110/435 [00:18<01:12,  4.46it/s] 26%|██▌       | 111/435 [00:18<01:27,  3.69it/s] 26%|██▌       | 112/435 [00:18<01:15,  4.27it/s] 26%|██▌       | 113/435 [00:19<01:26,  3.73it/s] 26%|██▌       | 114/435 [00:19<01:15,  4.28it/s] 26%|██▋       | 115/435 [00:19<01:29,  3.59it/s] 27%|██▋       | 116/435 [00:19<01:16,  4.15it/s] 27%|██▋       | 117/435 [00:20<01:31,  3.49it/s] 27%|██▋       | 118/435 [00:20<01:18,  4.05it/s] 27%|██▋       | 119/435 [00:20<01:18,  4.03it/s] 28%|██▊       | 120/435 [00:20<01:09,  4.56it/s] 28%|██▊       | 121/435 [00:21<01:12,  4.36it/s] 28%|██▊       | 122/435 [00:21<01:04,  4.85it/s] 28%|██▊       | 123/435 [00:21<01:08,  4.52it/s] 29%|██▊       | 124/435 [00:21<01:02,  4.99it/s] 29%|██▊       | 125/435 [00:22<01:07,  4.59it/s] 29%|██▉       | 126/435 [00:22<01:01,  5.04it/s] 29%|██▉       | 127/435 [00:22<01:11,  4.31it/s] 29%|██▉       | 128/435 [00:22<01:04,  4.79it/s] 30%|██▉       | 129/435 [00:22<01:07,  4.51it/s] 30%|██▉       | 130/435 [00:23<01:01,  4.96it/s] 30%|███       | 131/435 [00:23<01:06,  4.55it/s] 30%|███       | 132/435 [00:23<01:06,  4.57it/s] 31%|███       | 133/435 [00:23<01:06,  4.57it/s] 31%|███       | 134/435 [00:24<01:20,  3.75it/s] 31%|███       | 135/435 [00:24<01:16,  3.94it/s] 31%|███▏      | 136/435 [00:24<01:12,  4.11it/s] 31%|███▏      | 137/435 [00:24<01:11,  4.17it/s] 32%|███▏      | 138/435 [00:24<01:09,  4.26it/s] 32%|███▏      | 139/435 [00:25<01:08,  4.33it/s] 32%|███▏      | 140/435 [00:25<01:20,  3.64it/s] 32%|███▏      | 141/435 [00:25<01:18,  3.76it/s] 33%|███▎      | 142/435 [00:26<01:32,  3.17it/s] 33%|███▎      | 143/435 [00:26<01:24,  3.48it/s] 33%|███▎      | 144/435 [00:26<01:18,  3.70it/s] 33%|███▎      | 145/435 [00:26<01:14,  3.89it/s] 34%|███▎      | 146/435 [00:27<01:11,  4.02it/s] 34%|███▍      | 147/435 [00:27<01:09,  4.12it/s] 34%|███▍      | 148/435 [00:27<01:08,  4.19it/s] 34%|███▍      | 149/435 [00:27<01:07,  4.23it/s] 34%|███▍      | 150/435 [00:28<01:06,  4.27it/s] 35%|███▍      | 151/435 [00:28<01:06,  4.29it/s] 35%|███▍      | 152/435 [00:28<01:05,  4.30it/s] 35%|███▌      | 153/435 [00:28<01:05,  4.31it/s] 35%|███▌      | 154/435 [00:29<01:05,  4.30it/s] 36%|███▌      | 155/435 [00:29<01:09,  4.06it/s] 36%|███▌      | 156/435 [00:29<01:11,  3.89it/s] 36%|███▌      | 157/435 [00:29<01:14,  3.74it/s] 36%|███▋      | 158/435 [00:30<01:34,  2.94it/s] 37%|███▋      | 159/435 [00:30<01:30,  3.06it/s] 37%|███▋      | 160/435 [00:31<01:44,  2.64it/s] 37%|███▋      | 161/435 [00:31<01:39,  2.75it/s] 37%|███▋      | 162/435 [00:32<01:49,  2.49it/s] 37%|███▋      | 163/435 [00:32<01:40,  2.69it/s] 38%|███▊      | 164/435 [00:32<01:50,  2.45it/s] 38%|███▊      | 165/435 [00:33<01:41,  2.66it/s] 38%|███▊      | 166/435 [00:33<01:51,  2.41it/s] 38%|███▊      | 167/435 [00:33<01:42,  2.62it/s] 39%|███▊      | 168/435 [00:34<01:35,  2.79it/s] 39%|███▉      | 169/435 [00:34<01:31,  2.92it/s] 39%|███▉      | 170/435 [00:34<01:27,  3.02it/s] 39%|███▉      | 171/435 [00:35<01:25,  3.09it/s] 40%|███▉      | 172/435 [00:35<01:23,  3.13it/s] 40%|███▉      | 173/435 [00:35<01:22,  3.17it/s] 40%|████      | 174/435 [00:36<01:21,  3.19it/s] 40%|████      | 175/435 [00:36<01:21,  3.20it/s] 40%|████      | 176/435 [00:36<01:20,  3.20it/s] 41%|████      | 177/435 [00:36<01:20,  3.20it/s] 41%|████      | 178/435 [00:37<01:20,  3.18it/s] 41%|████      | 179/435 [00:37<01:20,  3.20it/s] 41%|████▏     | 180/435 [00:37<01:19,  3.19it/s] 42%|████▏     | 181/435 [00:38<01:19,  3.18it/s] 42%|████▏     | 182/435 [00:38<01:20,  3.15it/s] 42%|████▏     | 183/435 [00:38<01:20,  3.14it/s] 42%|████▏     | 184/435 [00:39<01:20,  3.14it/s] 43%|████▎     | 185/435 [00:39<01:20,  3.12it/s] 43%|████▎     | 186/435 [00:39<01:19,  3.12it/s] 43%|████▎     | 187/435 [00:40<01:15,  3.30it/s] 43%|████▎     | 188/435 [00:40<01:11,  3.44it/s] 43%|████▎     | 189/435 [00:40<01:09,  3.55it/s] 44%|████▎     | 190/435 [00:40<01:07,  3.61it/s] 44%|████▍     | 191/435 [00:41<01:06,  3.64it/s] 44%|████▍     | 192/435 [00:41<01:05,  3.68it/s] 44%|████▍     | 193/435 [00:41<01:05,  3.70it/s] 45%|████▍     | 194/435 [00:41<01:04,  3.72it/s] 45%|████▍     | 195/435 [00:42<01:05,  3.69it/s] 45%|████▌     | 196/435 [00:42<01:04,  3.68it/s] 45%|████▌     | 197/435 [00:42<01:05,  3.66it/s] 46%|████▌     | 198/435 [00:43<01:04,  3.65it/s] 46%|████▌     | 199/435 [00:43<01:04,  3.64it/s] 46%|████▌     | 200/435 [00:43<01:04,  3.64it/s] 46%|████▌     | 201/435 [00:43<01:04,  3.61it/s]Traceback (most recent call last):
  File "/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization.py", line 289, in <module>
    main(model_name, output_dir, scratch_dir, summarization_train_filepath, unique_save_name, dataset_prop, summarization_val_filepath, summarization_test_filepath, batch_size, eval_steps)
  File "/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization.py", line 232, in main
    benchmark_metrics = summarization_trainer.evaluate()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 3365, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 3580, in evaluation_loop
    preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)
                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 138, in nested_concat
    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 138, in <genexpr>
    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 140, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer_pt_utils.py", line 99, in torch_pad_and_concatenate
    return torch.cat((tensor1, tensor2), dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.43 GiB. GPU 0 has a total capacity of 79.10 GiB of which 8.38 GiB is free. Including non-PyTorch memory, this process has 70.71 GiB memory in use. Of the allocated memory 68.90 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
 46%|████▌     | 201/435 [00:45<00:52,  4.44it/s]
