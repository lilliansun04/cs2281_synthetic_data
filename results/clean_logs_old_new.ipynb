{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# NO EXCEPTION after standardizing pipeline\n",
    "# exception_file = \"t5-small_1_no_eval_train.csv\"\n",
    "summarization_folder = \"summarization/\"\n",
    "qna_folder = \"qna/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: t5-small_1_no_eval_2_train.csv\n",
      "    step    loss\n",
      "16   680  1.2126\n",
      "17   720  1.2045\n",
      "18   760  1.2041\n",
      "19   800  1.2004\n",
      "20   840  1.2036\n",
      "File: t5-base_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "16   680  0.4827\n",
      "17   720  0.4795\n",
      "18   760  0.4782\n",
      "19   800  0.4870\n",
      "20   840  0.4808\n",
      "File: t5-small_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "16   680  0.6073\n",
      "17   720  0.5974\n",
      "18   760  0.5971\n",
      "19   800  0.6035\n",
      "20   840  0.5948\n",
      "File: t5-small_1_no_eval_3_train.csv\n",
      "    step    loss\n",
      "16   680  1.2126\n",
      "17   720  1.2045\n",
      "18   760  1.2041\n",
      "19   800  1.2004\n",
      "20   840  1.2036\n",
      "File: t5-base_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "16   680  0.4827\n",
      "17   720  0.4795\n",
      "18   760  0.4782\n",
      "19   800  0.4870\n",
      "20   840  0.4808\n",
      "File: t5-base_1_no_eval_fast_2_train.csv\n",
      "    step    loss\n",
      "16   680  0.9135\n",
      "17   720  0.9112\n",
      "18   760  0.9137\n",
      "19   800  0.9137\n",
      "20   840  0.9173\n",
      "File: t5-base_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "16   680  0.4827\n",
      "17   720  0.4795\n",
      "18   760  0.4782\n",
      "19   800  0.4870\n",
      "20   840  0.4808\n",
      "File: t5-large_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "16   680  0.4082\n",
      "17   720  0.4056\n",
      "18   760  0.4023\n",
      "19   800  0.4116\n",
      "20   840  0.4051\n",
      "File: t5-small_1_no_eval_train.csv\n",
      "Skipping t5-small_1_no_eval_train.csv\n",
      "File: t5-small_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "16   680  0.6073\n",
      "17   720  0.5974\n",
      "18   760  0.5971\n",
      "19   800  0.6035\n",
      "20   840  0.5948\n",
      "File: t5-large_1_no_eval_train.csv\n",
      "    step    loss\n",
      "16   680  0.7134\n",
      "17   720  0.7132\n",
      "18   760  0.7146\n",
      "19   800  0.7120\n",
      "20   840  0.7171\n",
      "File: t5-large_1_no_eval_2_train.csv\n",
      "    step    loss\n",
      "16   680  0.7134\n",
      "17   720  0.7132\n",
      "18   760  0.7146\n",
      "19   800  0.7120\n",
      "20   840  0.7171\n",
      "File: t5-large_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "16   680  0.4082\n",
      "17   720  0.4056\n",
      "18   760  0.4023\n",
      "19   800  0.4116\n",
      "20   840  0.4051\n",
      "File: t5-small_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "16   680  0.6073\n",
      "17   720  0.5974\n",
      "18   760  0.5971\n",
      "19   800  0.6035\n",
      "20   840  0.5948\n",
      "File: t5-base_1_no_eval_fast_3_train.csv\n",
      "    step    loss\n",
      "16   680  0.9135\n",
      "17   720  0.9112\n",
      "18   760  0.9137\n",
      "19   800  0.9137\n",
      "20   840  0.9173\n",
      "File: t5-large_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "16   680  0.4082\n",
      "17   720  0.4056\n",
      "18   760  0.4023\n",
      "19   800  0.4116\n",
      "20   840  0.4051\n",
      "File: t5-large_1_no_eval_3_train.csv\n",
      "    step    loss\n",
      "16   680  0.7134\n",
      "17   720  0.7132\n",
      "18   760  0.7146\n",
      "19   800  0.7120\n",
      "20   840  0.7171\n",
      "File: t5-base_1_no_eval_fast_train.csv\n",
      "    step    loss\n",
      "16   680  0.9135\n",
      "17   720  0.9112\n",
      "18   760  0.9137\n",
      "19   800  0.9137\n",
      "20   840  0.9173\n"
     ]
    }
   ],
   "source": [
    "# for every file in the summarization folder, read in the csv file and get the file name\n",
    "for filename in os.listdir(summarization_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(summarization_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"File: {filename}\")\n",
    "        #keep only step and loss columns\n",
    "        df = df[['step', 'loss']].dropna()\n",
    "        print(df.tail())\n",
    "        df.to_csv(summarization_folder+\"cleaned/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: t5-small_1_no_eval_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3542\n",
      "27  1120  0.3148\n",
      "28  1160  0.3415\n",
      "29  1200  0.3156\n",
      "30  1240  0.3261\n",
      "File: t5-base_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3640\n",
      "27  1120  0.3502\n",
      "28  1160  0.3604\n",
      "29  1200  0.3233\n",
      "30  1240  0.3768\n",
      "File: t5-small_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.4651\n",
      "27  1120  0.4418\n",
      "28  1160  0.4366\n",
      "29  1200  0.4391\n",
      "30  1240  0.4786\n",
      "File: t5-small_1_no_eval_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3696\n",
      "27  1120  0.3267\n",
      "28  1160  0.3417\n",
      "29  1200  0.3226\n",
      "30  1240  0.3393\n",
      "File: t5-base_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3710\n",
      "27  1120  0.3464\n",
      "28  1160  0.3592\n",
      "29  1200  0.3287\n",
      "30  1240  0.3838\n",
      "File: t5-base_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3702\n",
      "27  1120  0.3483\n",
      "28  1160  0.3649\n",
      "29  1200  0.3284\n",
      "30  1240  0.3783\n",
      "File: t5-large_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3129\n",
      "27  1120  0.2688\n",
      "28  1160  0.2781\n",
      "29  1200  0.2532\n",
      "30  1240  0.2942\n",
      "File: t5-base_1_no_eval_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.2342\n",
      "27  1120  0.1711\n",
      "28  1160  0.1759\n",
      "29  1200  0.1326\n",
      "30  1240  0.1867\n",
      "File: t5-small_1_no_eval_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3539\n",
      "27  1120  0.3150\n",
      "28  1160  0.3301\n",
      "29  1200  0.3180\n",
      "30  1240  0.3257\n",
      "File: t5-small_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.4752\n",
      "27  1120  0.4554\n",
      "28  1160  0.4462\n",
      "29  1200  0.4461\n",
      "30  1240  0.4754\n",
      "File: t5-base_1_no_eval_train.csv\n",
      "    step    loss\n",
      "26  1080  0.2254\n",
      "27  1120  0.1730\n",
      "28  1160  0.1747\n",
      "29  1200  0.1332\n",
      "30  1240  0.1831\n",
      "File: t5-large_1_no_eval_train.csv\n",
      "    step    loss\n",
      "26  1080  0.1542\n",
      "27  1120  0.1072\n",
      "28  1160  0.1123\n",
      "29  1200  0.0931\n",
      "30  1240  0.1092\n",
      "File: t5-large_1_no_eval_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.1522\n",
      "27  1120  0.1116\n",
      "28  1160  0.1052\n",
      "29  1200  0.0905\n",
      "30  1240  0.1066\n",
      "File: t5-large_1_no_eval_human_2_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3115\n",
      "27  1120  0.2720\n",
      "28  1160  0.2856\n",
      "29  1200  0.2475\n",
      "30  1240  0.2876\n",
      "File: t5-small_1_no_eval_human_train.csv\n",
      "    step    loss\n",
      "26  1080  0.4845\n",
      "27  1120  0.4642\n",
      "28  1160  0.4570\n",
      "29  1200  0.4561\n",
      "30  1240  0.4744\n",
      "File: t5-base_1_no_eval_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.2262\n",
      "27  1120  0.1696\n",
      "28  1160  0.1717\n",
      "29  1200  0.1288\n",
      "30  1240  0.1856\n",
      "File: t5-large_1_no_eval_human_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.3159\n",
      "27  1120  0.2748\n",
      "28  1160  0.2711\n",
      "29  1200  0.2408\n",
      "30  1240  0.2863\n",
      "File: t5-large_1_no_eval_3_train.csv\n",
      "    step    loss\n",
      "26  1080  0.1517\n",
      "27  1120  0.1104\n",
      "28  1160  0.1114\n",
      "29  1200  0.0929\n",
      "30  1240  0.1113\n"
     ]
    }
   ],
   "source": [
    "# for every file in the summarization folder, read in the csv file and get the file name\n",
    "for filename in os.listdir(qna_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(qna_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"File: {filename}\")\n",
    "        #keep only step and loss columns\n",
    "        df = df[['step', 'loss']].dropna()\n",
    "        print(df.tail())\n",
    "        df.to_csv(qna_folder+\"cleaned/\"+filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
