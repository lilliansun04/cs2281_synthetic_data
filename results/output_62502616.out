/n/holylabs/LABS/hlakkaraju_lab/Users/lilliansun/cs2281_synthetic_data /n/holylabs/LABS/hlakkaraju_lab/Users/lilliansun/cs2281_synthetic_data
/n/holylabs/LABS/hlakkaraju_lab/Users/lilliansun/cs2281_synthetic_data /n/holylabs/LABS/hlakkaraju_lab/Users/lilliansun/cs2281_synthetic_data /n/holylabs/LABS/hlakkaraju_lab/Users/lilliansun/cs2281_synthetic_data
Running finetuning_qna_no_eval_human.py
Saving results to results/qna/t5-small_1_no_eval_human_2
Number of GPUs available: 1
GPU 0: NVIDIA H100 80GB HBM3
Model name: google/flan-t5-small
Output directory: results/qna/
Scratch directory: /n/netscratch/hlakkaraju_lab/Everyone/lilliansun/synthetic_data/qna/
Unique save name: t5-small_1_no_eval_human_2
Dataset proportion: 1.0
Q&A train filepath: synthetic/qna_train.csv
Q&A val filepath: synthetic/qna_val.csv
Q&A test filepath: synthetic/qna_test.csv
Batch size: 8
Evaluation steps: 40
Size of input_ids tensor: torch.Size([393]), dtype: torch.int64
Actual sequence length: 80
Size of input_ids tensor: torch.Size([245]), dtype: torch.int64
Actual sequence length: 124
Size of input_ids tensor: torch.Size([393]), dtype: torch.int64
Actual sequence length: 100
{'loss': 0.5929, 'grad_norm': 1.7031277418136597, 'learning_rate': 4.839615076182839e-05, 'epoch': 0.03}
{'loss': 0.5502, 'grad_norm': 1.7813833951950073, 'learning_rate': 4.679230152365678e-05, 'epoch': 0.06}
{'loss': 0.5651, 'grad_norm': 1.05954110622406, 'learning_rate': 4.518845228548517e-05, 'epoch': 0.1}
{'loss': 0.5371, 'grad_norm': 0.5777654051780701, 'learning_rate': 4.358460304731356e-05, 'epoch': 0.13}
{'loss': 0.5317, 'grad_norm': 2.569887399673462, 'learning_rate': 4.1980753809141946e-05, 'epoch': 0.16}
{'loss': 0.5538, 'grad_norm': 1.0589817762374878, 'learning_rate': 4.037690457097033e-05, 'epoch': 0.19}
{'loss': 0.5484, 'grad_norm': 0.769906222820282, 'learning_rate': 3.8773055332798716e-05, 'epoch': 0.22}
{'loss': 0.5429, 'grad_norm': 0.8153451681137085, 'learning_rate': 3.7169206094627105e-05, 'epoch': 0.26}
{'loss': 0.5404, 'grad_norm': 0.6649954319000244, 'learning_rate': 3.5565356856455494e-05, 'epoch': 0.29}
{'loss': 0.5253, 'grad_norm': 2.76418137550354, 'learning_rate': 3.396150761828388e-05, 'epoch': 0.32}
{'loss': 0.5354, 'grad_norm': 1.9037976264953613, 'learning_rate': 3.235765838011227e-05, 'epoch': 0.35}
{'loss': 0.5325, 'grad_norm': 0.9526215195655823, 'learning_rate': 3.075380914194066e-05, 'epoch': 0.38}
{'loss': 0.5217, 'grad_norm': 1.874537706375122, 'learning_rate': 2.914995990376905e-05, 'epoch': 0.42}
{'loss': 0.5177, 'grad_norm': 2.192528009414673, 'learning_rate': 2.7546110665597434e-05, 'epoch': 0.45}
{'loss': 0.4849, 'grad_norm': 1.9953728914260864, 'learning_rate': 2.5942261427425823e-05, 'epoch': 0.48}
{'loss': 0.5104, 'grad_norm': 1.9465166330337524, 'learning_rate': 2.433841218925421e-05, 'epoch': 0.51}
{'loss': 0.4846, 'grad_norm': 1.8140788078308105, 'learning_rate': 2.27345629510826e-05, 'epoch': 0.55}
{'loss': 0.4923, 'grad_norm': 2.527073383331299, 'learning_rate': 2.113071371291099e-05, 'epoch': 0.58}
{'loss': 0.4755, 'grad_norm': 2.4260079860687256, 'learning_rate': 1.9526864474739377e-05, 'epoch': 0.61}
{'loss': 0.4675, 'grad_norm': 1.8812710046768188, 'learning_rate': 1.7923015236567763e-05, 'epoch': 0.64}
{'loss': 0.4368, 'grad_norm': 2.3232738971710205, 'learning_rate': 1.631916599839615e-05, 'epoch': 0.67}
{'loss': 0.4693, 'grad_norm': 3.329615354537964, 'learning_rate': 1.471531676022454e-05, 'epoch': 0.71}
{'loss': 0.4861, 'grad_norm': 3.1300907135009766, 'learning_rate': 1.3111467522052929e-05, 'epoch': 0.74}
{'loss': 0.4719, 'grad_norm': 2.475240707397461, 'learning_rate': 1.1507618283881316e-05, 'epoch': 0.77}
{'loss': 0.4613, 'grad_norm': 2.1724653244018555, 'learning_rate': 9.903769045709705e-06, 'epoch': 0.8}
{'loss': 0.4606, 'grad_norm': 2.2208328247070312, 'learning_rate': 8.299919807538093e-06, 'epoch': 0.83}
{'loss': 0.4752, 'grad_norm': 2.0794875621795654, 'learning_rate': 6.69607056936648e-06, 'epoch': 0.87}
{'loss': 0.4554, 'grad_norm': 2.3411612510681152, 'learning_rate': 5.092221331194868e-06, 'epoch': 0.9}
{'loss': 0.4462, 'grad_norm': 2.6428275108337402, 'learning_rate': 3.488372093023256e-06, 'epoch': 0.93}
{'loss': 0.4461, 'grad_norm': 1.6838479042053223, 'learning_rate': 1.884522854851644e-06, 'epoch': 0.96}
{'loss': 0.4754, 'grad_norm': 2.7634851932525635, 'learning_rate': 2.8067361668003206e-07, 'epoch': 0.99}
{'train_runtime': 407.5776, 'train_samples_per_second': 97.935, 'train_steps_per_second': 3.06, 'train_loss': 0.5029631135171189, 'epoch': 1.0}
DONE
