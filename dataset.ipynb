{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "* Run the code starting from 'Dataset Creation' up to Synthetic Data Generation. You only need to do this once\n",
    "* Then, the 'Synthetic Data Generation' section will contain everything you need for generating synthetic data from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "1) Download the CNN/Dailymail dataset into the folder `datasets/`. The folder should be named `cnn_dailymail` already, and the train `.csv` should be in `datasets/cnn_dailymail/train.csv` (or change the directory below as needed)\n",
    "2) create the directory `datasets/cnn_parsed`\n",
    "3) Run this section of the notebook. This should remove all the dailymail and duplicate articles and save the new train test val split into the folder above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/cnn_dailymail/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 16\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m df_test \u001b[38;5;241m=\u001b[39m load_and_parse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m df_val \u001b[38;5;241m=\u001b[39m load_and_parse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m, in \u001b[0;36mload_and_parse\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_parse\u001b[39m(dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39marticle\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/cnn_dailymail/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_dir_in = 'datasets/cnn_dailymail/'\n",
    "dataset_dir_out = 'datasets/cnn_parsed/'\n",
    "\n",
    "# load train dataset\n",
    "def load_and_parse(dataset = 'train'):\n",
    "    df = pd.read_csv(dataset_dir_in + dataset + '.csv')\n",
    "    df = df[df.article.str.contains('CNN')]\n",
    "    df = df.drop_duplicates('article')\n",
    "    return df\n",
    "\n",
    "df_train = load_and_parse('train')\n",
    "df_test = load_and_parse('test')\n",
    "df_val = load_and_parse('validation')\n",
    "\n",
    "df = pd.concat([df_train, df_val, df_test])\n",
    "\n",
    "num_articles_per_split = 128 # 128\n",
    "\n",
    "def create_splits(df, num_articles_per_split):\n",
    "    df = df.sort_values('id')\n",
    "    splits = np.array(range(df.shape[0]))\n",
    "    splits = splits // num_articles_per_split\n",
    "    df['split'] = splits\n",
    "    return df\n",
    "\n",
    "df = create_splits(df, num_articles_per_split)\n",
    "\n",
    "df.to_csv(dataset_dir_out + 'all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, make sure the directories match with the ones you created\n",
    "* Then specify 'splits' to choose which splits you want to generate data for. Kerem: 0-150, Lillian: 151-300, Emma: 301-450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "task = 'qna' # qna or summary\n",
    "\n",
    "out_dir = f'datasets/synthetic/{task}/'\n",
    "\n",
    "# creates lookup table for data already processed\n",
    "def create_lookup_table(dir = out_dir):\n",
    "    files = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "\n",
    "    lookup_df = None\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        loc = dir + f\n",
    "        if lookup_df is None:\n",
    "            lookup_df = pd.read_csv(loc)\n",
    "        else:\n",
    "            df2 = pd.read_csv(loc)\n",
    "            lookup_df = pd.concat([lookup_df, df2])\n",
    "\n",
    "    if lookup_df is None:\n",
    "        return {}\n",
    "\n",
    "    unique_ids = lookup_df.id.unique()\n",
    "    id_lookup = {}\n",
    "    for idx in unique_ids:\n",
    "        id_lookup[idx] = 1\n",
    "\n",
    "    return id_lookup\n",
    "\n",
    "id_lookup = create_lookup_table(dir = out_dir)\n",
    "\n",
    "def is_processed(row):\n",
    "    try:\n",
    "        idx = id_lookup[row['id']]\n",
    "        return False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL DATA\n",
      "83558\n",
      "REMAINING\n",
      "(83558, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5223"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data and remove all ids that are already processed\n",
    "df_all = pd.read_csv('datasets/cnn_parsed/all.csv')\n",
    "\n",
    "print('ALL DATA')\n",
    "print(df_all.shape[0])\n",
    "\n",
    "df_all = df_all[df_all.apply(is_processed, axis = 1)]\n",
    "print('REMAINING')\n",
    "print(df_all.shape)\n",
    "\n",
    "synthetic_data_dir = f'datasets/synthetic/{task}/'\n",
    "\n",
    "if task == 'summary':\n",
    "    generator = data.SummaryGenerator()\n",
    "else:\n",
    "    generator = data.QnAGenerator()\n",
    "\n",
    "total_splits = df_all.split.max() + 1\n",
    "\n",
    "total_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we make the API requests and parse the output in real time. For batch processing, see the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Parsing split 0 -----\n",
      "----- Number of articles to parse: 16 -----\n",
      "----- ELAPSED TIME -----\n",
      "77.4 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ! This is for real time processing ! \n",
    "# ! See below for batches !\n",
    "#\n",
    "splits = list(range(0, 1))\n",
    "data.process_splits(df_all, generator, splits, synthetic_data_dir = synthetic_data_dir, mode = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create batch api (instead of the above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially create the `.jsonl` file which will be uploaded to OpenAI files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"batch_requests_{task}.jsonl\", \"w\") as f:\n",
    "    for i , (_, row) in enumerate(df_all.iterrows()):\n",
    "        if i == 10000:\n",
    "            break\n",
    "\n",
    "        text = row['article']\n",
    "        idx = row['id']\n",
    "\n",
    "        id_lookup[idx] = 1\n",
    "\n",
    "        custom_id = f'request-{task}-{idx}'\n",
    "\n",
    "        system_prompt = generator.system_prompt\n",
    "        \n",
    "        messages = [\n",
    "            {'role' : 'system', 'content' : system_prompt},\n",
    "            {'role' : 'user', 'content' : text}\n",
    "        ]\n",
    "\n",
    "        body = {'model' : 'gpt-4o-mini', 'messages' : messages, 'max_tokens' : 2048, 'temperature' : 0.1}\n",
    "        \n",
    "        line = {'custom_id' : custom_id, 'method' : 'POST', 'url' : '/v1/chat/completions', 'body' : body}\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, upload the file created above with all the requests to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "file=open(f\"batch_requests_{task}.jsonl\", \"rb\"),\n",
    "purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the batch processing. Make sure the following outputs `gpt-4o-mini` and `gpt-4o-mini-2024-07-18` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id='batch_675b4c357e688190b7147f69fc698aab', completion_window='24h', created_at=1734036533, endpoint='/v1/chat/completions', input_file_id='file-C1tn1nDmPATA7B5gd2TQU8', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1734122933, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Summarization'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "client.batches.create(\n",
    "  input_file_id=batch_input_file.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\",\n",
    "  metadata={\n",
    "    \"description\": f\"{task}\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse batch responses\n",
    "Once the batch processing is done (check through the OpenAI platform dashboard), download the output file to a proper location and parse the output using the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m---> 35\u001b[0m df_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'datasets/batch_675b4c357e688190b7147f69fc698aab_output.jsonl'\n",
    "\n",
    "generator = data.SummaryGenerator() if task == 'summary' else data.QnAGenerator()\n",
    "\n",
    "with open(filename, 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "\n",
    "results = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "\n",
    "    id = result['custom_id'].split('-')[2]\n",
    "    text = result['response']['body']['choices'][0]['message']['content']\n",
    "    \n",
    "    try:\n",
    "        out = generator.parse_response(text)\n",
    "       \n",
    "        out['id'] = id\n",
    "        out['gpt_summary'] = out.pop('summary')\n",
    "\n",
    "        if task == 'summary':\n",
    "            out['gpt_keywords'] = out.pop('keywords')\n",
    "        else:\n",
    "            out['qna'] = out.pop('qna')\n",
    "        results.append(out)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "        print(e)\n",
    "\n",
    "df_out = pd.DataFrame(columns = results[0].keys())\n",
    "for result in results:\n",
    "    for key in result.keys():\n",
    "        result[key] = [result[key]] # otherwise the lists are ignored\n",
    "    df_row = pd.DataFrame.from_dict(result)\n",
    "    df_out = pd.concat([df_out, df_row])\n",
    "    \n",
    "df_all = pd.read_csv('datasets/cnn_parsed/all.csv')\n",
    "df_all = df_all.rename(columns={'highlights' : 'human_summary'})\n",
    "\n",
    "df_merged = pd.merge(df_all, df_out, on='id', how='inner')\n",
    "df_merged.drop(df_merged.columns[df_merged.columns.str.contains('^Unnamed')], axis=1, inplace=True)\n",
    "print(df_merged.shape)\n",
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outputs\n",
    "# make sure you change the filename below every time you run batch to avoid overriding\n",
    "df_merged.to_csv(f'datasets/synthetic/{task}/batch_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_0.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>human_summary</th>\n",
       "      <th>gpt_summary</th>\n",
       "      <th>qna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000128cbd36642ced67ac90bd7d4d1dd5e8cf554</td>\n",
       "      <td>December 19, 2014 . CNN Student News is wrappi...</td>\n",
       "      <td>This page includes the show Transcript .\\nUse ...</td>\n",
       "      <td>CNN Student News is concluding 2014 with a re...</td>\n",
       "      <td>[{\"Q\": \" Did CNN Student News cover ten intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f1fcec4ca8bc7e278607ba0e31e5cc046e66</td>\n",
       "      <td>Democratic Republic of Congo (CNN) -- Our frie...</td>\n",
       "      <td>Ashley Judd  tells story of girl who was victi...</td>\n",
       "      <td>The text discusses the plight of Kika, a surv...</td>\n",
       "      <td>[{\"Q\": \" Is Kika a survivor of gender violence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "      <td>Ralph Mata, a lieutenant in the Miami-Dade Po...</td>\n",
       "      <td>[{\"Q\": \" Was Ralph Mata a police officer?\\n\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "      <td>President Vladimir Putin has committed Crimea...</td>\n",
       "      <td>[{\"Q\": \" Did President Putin commit Crimea to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000424152bce9d9f36cb43884dacf16b43052463</td>\n",
       "      <td>(CNN) -- The powerful but compact Hurricane Ra...</td>\n",
       "      <td>NEW: Raymond weakens slightly as it moves towa...</td>\n",
       "      <td>Hurricane Raymond, a Category 3 storm with wi...</td>\n",
       "      <td>[{\"Q\": \" Is Hurricane Raymond currently a Cate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  000128cbd36642ced67ac90bd7d4d1dd5e8cf554   \n",
       "1  0001f1fcec4ca8bc7e278607ba0e31e5cc046e66   \n",
       "2  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  000424152bce9d9f36cb43884dacf16b43052463   \n",
       "\n",
       "                                             article  \\\n",
       "0  December 19, 2014 . CNN Student News is wrappi...   \n",
       "1  Democratic Republic of Congo (CNN) -- Our frie...   \n",
       "2  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  (CNN) -- The powerful but compact Hurricane Ra...   \n",
       "\n",
       "                                       human_summary  \\\n",
       "0  This page includes the show Transcript .\\nUse ...   \n",
       "1  Ashley Judd  tells story of girl who was victi...   \n",
       "2  Criminal complaint: Cop used his role to help ...   \n",
       "3  Nina dos Santos says Europe must be ready to a...   \n",
       "4  NEW: Raymond weakens slightly as it moves towa...   \n",
       "\n",
       "                                         gpt_summary  \\\n",
       "0   CNN Student News is concluding 2014 with a re...   \n",
       "1   The text discusses the plight of Kika, a surv...   \n",
       "2   Ralph Mata, a lieutenant in the Miami-Dade Po...   \n",
       "3   President Vladimir Putin has committed Crimea...   \n",
       "4   Hurricane Raymond, a Category 3 storm with wi...   \n",
       "\n",
       "                                                 qna  \n",
       "0  [{\"Q\": \" Did CNN Student News cover ten intern...  \n",
       "1  [{\"Q\": \" Is Kika a survivor of gender violence...  \n",
       "2  [{\"Q\": \" Was Ralph Mata a police officer?\\n\", ...  \n",
       "3  [{\"Q\": \" Did President Putin commit Crimea to ...  \n",
       "4  [{\"Q\": \" Is Hurricane Raymond currently a Cate...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import data\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "dir = f'datasets/synthetic/{task}/'\n",
    "\n",
    "files = [f for f in listdir(dir) if isfile(join(dir, f))]\n",
    "\n",
    "df = None\n",
    "for f in files:\n",
    "    print(f)\n",
    "    loc = dir + f\n",
    "    if df is None:\n",
    "        df = pd.read_csv(loc)\n",
    "    else:\n",
    "        df2 = pd.read_csv(loc)\n",
    "        df = pd.concat([df, df2])\n",
    "\n",
    "df.drop(df.columns[df.columns.str.contains('^split')], axis = 1, inplace=True)\n",
    "df.drop(df.columns[df.columns.str.contains('^Unnamed')], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>human_summary</th>\n",
       "      <th>gpt_summary</th>\n",
       "      <th>qna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000128cbd36642ced67ac90bd7d4d1dd5e8cf554</td>\n",
       "      <td>December 19, 2014 . CNN Student News is wrappi...</td>\n",
       "      <td>This page includes the show Transcript .\\nUse ...</td>\n",
       "      <td>CNN Student News is concluding 2014 with a re...</td>\n",
       "      <td>[{\"Q\": \" Did CNN Student News cover ten intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f1fcec4ca8bc7e278607ba0e31e5cc046e66</td>\n",
       "      <td>Democratic Republic of Congo (CNN) -- Our frie...</td>\n",
       "      <td>Ashley Judd  tells story of girl who was victi...</td>\n",
       "      <td>The text discusses the plight of Kika, a surv...</td>\n",
       "      <td>[{\"Q\": \" Is Kika a survivor of gender violence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "      <td>Ralph Mata, a lieutenant in the Miami-Dade Po...</td>\n",
       "      <td>[{\"Q\": \" Was Ralph Mata a police officer?\\n\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "      <td>President Vladimir Putin has committed Crimea...</td>\n",
       "      <td>[{\"Q\": \" Did President Putin commit Crimea to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000424152bce9d9f36cb43884dacf16b43052463</td>\n",
       "      <td>(CNN) -- The powerful but compact Hurricane Ra...</td>\n",
       "      <td>NEW: Raymond weakens slightly as it moves towa...</td>\n",
       "      <td>Hurricane Raymond, a Category 3 storm with wi...</td>\n",
       "      <td>[{\"Q\": \" Is Hurricane Raymond currently a Cate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  000128cbd36642ced67ac90bd7d4d1dd5e8cf554   \n",
       "1  0001f1fcec4ca8bc7e278607ba0e31e5cc046e66   \n",
       "2  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
       "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
       "4  000424152bce9d9f36cb43884dacf16b43052463   \n",
       "\n",
       "                                             article  \\\n",
       "0  December 19, 2014 . CNN Student News is wrappi...   \n",
       "1  Democratic Republic of Congo (CNN) -- Our frie...   \n",
       "2  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  (CNN) -- The powerful but compact Hurricane Ra...   \n",
       "\n",
       "                                       human_summary  \\\n",
       "0  This page includes the show Transcript .\\nUse ...   \n",
       "1  Ashley Judd  tells story of girl who was victi...   \n",
       "2  Criminal complaint: Cop used his role to help ...   \n",
       "3  Nina dos Santos says Europe must be ready to a...   \n",
       "4  NEW: Raymond weakens slightly as it moves towa...   \n",
       "\n",
       "                                         gpt_summary  \\\n",
       "0   CNN Student News is concluding 2014 with a re...   \n",
       "1   The text discusses the plight of Kika, a surv...   \n",
       "2   Ralph Mata, a lieutenant in the Miami-Dade Po...   \n",
       "3   President Vladimir Putin has committed Crimea...   \n",
       "4   Hurricane Raymond, a Category 3 storm with wi...   \n",
       "\n",
       "                                                 qna  \n",
       "0  [{\"Q\": \" Did CNN Student News cover ten intern...  \n",
       "1  [{\"Q\": \" Is Kika a survivor of gender violence...  \n",
       "2  [{\"Q\": \" Was Ralph Mata a police officer?\\n\", ...  \n",
       "3  [{\"Q\": \" Did President Putin commit Crimea to ...  \n",
       "4  [{\"Q\": \" Is Hurricane Raymond currently a Cate...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "for col in df.columns:\n",
    "    print(df[col].unique().shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'datasets/synthetic/{task}_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Q': ' Did CNN Student News cover ten international stories in their report?  \\n',\n",
       "  'A': True},\n",
       " {'Q': ' Will CNN Student News resume its program on January 5, 2015?  \\n',\n",
       "  'A': True},\n",
       " {'Q': ' Is there a Weekly Newsquiz available on the page?  \\n', 'A': True},\n",
       " {'Q': ' Can students younger than 13 request to be mentioned in the CNN Student News Roll Call?  \\n',\n",
       "  'A': False},\n",
       " {'Q': ' Did the staff at CNN Student News thank their audience for the past year?  \\n',\n",
       "  'A': True}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json.loads(df['qna'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
