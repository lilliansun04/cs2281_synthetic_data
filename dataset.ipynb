{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "* Run the code starting from 'Dataset Creation' up to Synthetic Data Generation. You only need to do this once\n",
    "* Then, the 'Synthetic Data Generation' section will contain everything you need for generating synthetic data from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "1) Download the CNN/Dailymail dataset into the folder `datasets/`. The folder should be named `cnn_dailymail` already, and the train `.csv` should be in `datasets/cnn_dailymail/train.csv` (or change the directory below as needed)\n",
    "2) create the directory `datasets/cnn_parsed`\n",
    "3) Run this section of the notebook. This should remove all the dailymail and duplicate articles and save the new train test val split into the folder above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_dir_in = 'datasets/cnn_dailymail/'\n",
    "dataset_dir_out = 'datasets/cnn_parsed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "def load_and_parse(dataset = 'train'):\n",
    "    df = pd.read_csv(dataset_dir_in + dataset + '.csv')\n",
    "    df = df[df.article.str.contains('CNN')]\n",
    "    df = df.drop_duplicates('article')\n",
    "    return df\n",
    "\n",
    "df_train = load_and_parse('train')\n",
    "df_test = load_and_parse('test')\n",
    "df_val = load_and_parse('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_val])\n",
    "df_train, df_val = train_test_split(df, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_articles_per_split = 128\n",
    "\n",
    "def create_splits(df, num_articles_per_split):\n",
    "    df = df.sort_values('id')\n",
    "    splits = np.array(range(df.shape[0]))\n",
    "    splits = splits // num_articles_per_split\n",
    "    df['split'] = splits\n",
    "    return df\n",
    "\n",
    "df_train, df_val = create_splits(df_train, num_articles_per_split), create_splits(df_val, num_articles_per_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(dataset_dir_out + 'train.csv')\n",
    "df_val.to_csv(dataset_dir_out + 'val.csv')\n",
    "df_test.to_csv(dataset_dir_out + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, make sure the directories match with the ones you created\n",
    "* Then specify 'splits' to choose which splits you want to generate data for. Kerem: 0-150, Lillian: 151-300, Emma: 301-450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# test the summary generator\n",
    "\n",
    "df_train = pd.read_csv('datasets/cnn_parsed/train.csv')\n",
    "\n",
    "task = 'summary' # summary or qna\n",
    "synthetic_data_dir = f'datasets/synthetic/{task}/'\n",
    "\n",
    "if task == 'summary':\n",
    "    generator = data.SummaryGenerator()\n",
    "else:\n",
    "    generator = data.QnAGenerator()\n",
    "\n",
    "total_splits = df_train.split.max() + 1\n",
    "\n",
    "total_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Parsing split 5 -----\n",
      "----- Number of articles to parse: 128 -----\n",
      "----- ELAPSED TIME -----\n",
      "208.9 seconds\n",
      "----- Parsing split 6 -----\n",
      "----- Number of articles to parse: 128 -----\n",
      "----- ELAPSED TIME -----\n",
      "227.9 seconds\n",
      "----- Parsing split 7 -----\n",
      "----- Number of articles to parse: 128 -----\n",
      "----- ELAPSED TIME -----\n",
      "201.8 seconds\n",
      "----- Parsing split 8 -----\n",
      "----- Number of articles to parse: 128 -----\n",
      "----- ELAPSED TIME -----\n",
      "198.6 seconds\n",
      "----- Parsing split 9 -----\n",
      "----- Number of articles to parse: 128 -----\n",
      "----- ELAPSED TIME -----\n",
      "222.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "splits = [5,6,7,8,9]\n",
    "data.process_splits(df_train, generator, splits, synthetic_data_dir = synthetic_data_dir, mode = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
