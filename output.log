~/cs2281_synthetic_data ~/cs2281_synthetic_data
~/cs2281_synthetic_data ~/cs2281_synthetic_data ~/cs2281_synthetic_data
Running finetuning_summarization_no_eval.py
Saving results to results/summarization/t5-large_1_no_eval
Number of GPUs available: 1
GPU 0: NVIDIA H100 80GB HBM3
Model name: google/flan-t5-large
Output directory: results/summarization/
Scratch directory: /n/netscratch/hlakkaraju_lab/Everyone/lilliansun/synthetic_data/
Unique save name: t5-large_1_no_eval
Dataset proportion: 1.0
Summarization train filepath: synthetic/summary_train.csv
Summarization val filepath: synthetic/summary_val.csv
Summarization test filepath: synthetic/summary_test.csv
Batch size: 16
Evaluation steps: 20
Size of input_ids tensor: torch.Size([3000]), dtype: torch.int64
Actual sequence length: 542
Size of input_ids tensor: torch.Size([2736]), dtype: torch.int64
Actual sequence length: 293
Size of input_ids tensor: torch.Size([3000]), dtype: torch.int64
Actual sequence length: 965
/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/434 [00:00<?, ?it/s]/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
  0%|          | 1/434 [00:39<4:45:29, 39.56s/it]Traceback (most recent call last):
  File "/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization_no_eval.py", line 179, in <module>
    main(model_name, output_dir, scratch_dir, summarization_train_filepath, unique_save_name, dataset_prop, summarization_val_filepath, summarization_test_filepath, batch_size, eval_steps)
  File "/n/home11/lilliansun/cs2281_synthetic_data/finetuning_summarization_no_eval.py", line 143, in main
    summarization_trainer.train()
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 2118, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/transformers/trainer.py", line 3045, in training_step
    self.accelerator.backward(loss)
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/accelerate/accelerator.py", line 2013, in backward
    loss.backward(**kwargs)
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/autograd/function.py", line 289, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 319, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/n/home11/lilliansun/.conda/envs/jupyter_py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.58 GiB. GPU 0 has a total capacity of 79.22 GiB of which 2.38 GiB is free. Including non-PyTorch memory, this process has 76.83 GiB memory in use. Of the allocated memory 60.84 GiB is allocated by PyTorch, and 15.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 1/434 [00:53<6:27:26, 53.69s/it]
DONE
